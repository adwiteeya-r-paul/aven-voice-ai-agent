# -*- coding: utf-8 -*-
"""Data Processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14BfTV8uV9FpXApUzG2fxJ73KvlHDrfD5

# Install libraries
"""

! pip install langchain langchain-community openai groq tiktoken pinecone-client langchain_pinecone unstructured pdfminer==20191125 pdfminer.six==20221105 pillow_heif unstructured_inference sentence-transformers exa-py

from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, WebBaseLoader, YoutubeLoader, DirectoryLoader, TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sklearn.metrics.pairwise import cosine_similarity
from langchain_pinecone import PineconeVectorStore
from langchain.embeddings import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from google.colab import userdata
from langchain.schema import Document
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from openai import OpenAI
import numpy as np
import tiktoken
import os
from exa_py import Exa

pinecone_api_key = userdata.get("PINECONE_API")
os.environ['PINECONE_API'] = pinecone_api_key


groq_api_key = userdata.get("GROQ_API")
os.environ['GROQ_API'] = groq_api_key


exa_api_key = userdata.get("EXA_API")
os.environ['EXA_API'] = exa_api_key
exa = Exa(exa_api_key)

"""# Load in the Data"""

results = exa.search_and_contents(
    "Aven Support Articles",
    text = True,
)

text_content = ""
for line in results.results:
  text_content += line.text + "\n"

with open("Aven Support", "w") as f:
    f.write(text_content)

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 50,
    chunk_overlap  = 10,
    separators = ["\n-","\n\n"]
)

documents = text_splitter.create_documents([text_content])

"""# Initialize the HuggingFace Embeddings client"""

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

embed = []
for i,n in enumerate(documents):
  embed.append(embeddings.embed_query(documents[i].page_content))

print(embed)

"""# Initialize Pinecone"""

# Make sure to create a Pinecone index with 384 dimensions
from pinecone import ServerlessSpec

pc = Pinecone(api_key=os.getenv('PINECONE_API'))
index_name = "aven-agent"
dimension = 384
namespace = "aven documents"

if index_name not in pc.list_indexes().names():
  pc.create_index(
            name=index_name,
            dimension=dimension,
            metric='cosine',
            spec= ServerlessSpec(cloud='aws', region='us-east-1'),
        )

index = pc.Index(index_name)

vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings, pinecone_api_key=os.getenv('PINECONE_API'))

"""# Insert data into Pinecone"""

batch_size = 100
for i in range(0, len(documents), batch_size):
    i_end = min(i+batch_size, len(documents))
    batch = documents[i:i_end]
    metadatas = [{"text": doc.page_content} for doc in batch]
    texts = [doc.page_content for doc in batch]
    docsearch = PineconeVectorStore(index_name=index_name, embedding=embeddings, pinecone_api_key=os.getenv('PINECONE_API'))
    docsearch.add_texts(texts, metadatas=metadatas, namespace=namespace)

"""# Perform RAG




"""

query = "Who is the CEO of Aven?"

query_embeddings = embeddings.embed_query(text = query)

query_embeddings

top_matches = index.query(vector=query_embeddings, top_k=1, include_metadata=True, namespace=namespace)

print(top_matches)

contexts = [item['metadata']['text'] for item in top_matches['matches']]
contexts

# Free Llama 3.1 API via Groq
from groq import Groq
groq_client = Groq(api_key=os.getenv('GROQ_API'))

augmented_query = "<CONTEXT>\n" + "\n\n-------\n\n".join(contexts[ : 10]) + "\n-------\n</CONTEXT>\n\n\n\nMY QUESTION:\n" + query

system_prompt = f"""Your name is AvenVoix. Everytime user asks you something, you will greet the user and say thanks. And then you will respon with the information you have. Keep it casual. Ask if you can help them with anything else. Don't say anything about the context to the user. If you have related information, share that."
"""

llm_response = groq_client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": augmented_query}
    ]
)

response = llm_response.choices[0].message.content

print(response)